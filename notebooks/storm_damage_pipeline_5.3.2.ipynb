{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18588b25",
   "metadata": {},
   "source": [
    "\n",
    "# üå™Ô∏è Storm Damage Prediction ‚Äî v5.6 (FAST, GPU‚Äëaware)\n",
    "Predict **Property** and **Crop** damages from NOAA Storm Events using:\n",
    "- Text embeddings (SentenceTransformers, GPU‚Äëaware)\n",
    "- Numeric + Categorical features\n",
    "- Geospatial & light spatiotemporal context (fast network features)\n",
    "- **XGB Tweedie** with **Slim Optuna** tuning (GPU/CPU auto)\n",
    "- **Quantile XGB** for prediction intervals\n",
    "- **SHAP** for feature importance & explainability\n",
    "\n",
    "**Runtime modes:** set env var `STORM_MODE` to `2h` (default), `3h`, or `full`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe377d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE=2h | CUDA=True | OUTDIR=C:\\Users\\ducan\\Documents\\lmu\\LMUmaster\\sem3\\sem3Projekte\\DamagesPrediction5\\results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1) Setup & Utils ===\n",
    "import os, sys, math, json, time, gc, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "MODE = os.getenv(\"STORM_MODE\", \"2h\").lower()\n",
    "HAS_CUDA = False\n",
    "try:\n",
    "    import torch\n",
    "    HAS_CUDA = torch.cuda.is_available()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "OUTDIR = Path(\"./results\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "@contextmanager\n",
    "def timer(msg):\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    print(f\"‚è±Ô∏è {msg} ...\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        print(f\"‚è±Ô∏è {msg}: {time.time()-t0:.2f}s\")\n",
    "\n",
    "def save_json(obj, path):\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, indent=2)\n",
    "\n",
    "def save_joblib(obj, path):\n",
    "    import joblib\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(obj, path)\n",
    "\n",
    "print(f\"MODE={MODE} | CUDA={HAS_CUDA} | OUTDIR={OUTDIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3805e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned shape: (52259, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>Y_PROP</th>\n",
       "      <th>Y_CROP</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "      <th>LAT_MEAN</th>\n",
       "      <th>LON_MEAN</th>\n",
       "      <th>EVENT_SPAN_KM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201302</td>\n",
       "      <td>23</td>\n",
       "      <td>1900</td>\n",
       "      <td>201302</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>2013</td>\n",
       "      <td>February</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A coastal low moved southeast of southern New ...</td>\n",
       "      <td>Three to five inches of snow fell across easte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201312</td>\n",
       "      <td>14</td>\n",
       "      <td>2100</td>\n",
       "      <td>201312</td>\n",
       "      <td>15</td>\n",
       "      <td>1300</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>33</td>\n",
       "      <td>2013</td>\n",
       "      <td>December</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure moved out of the midwest, off the...</td>\n",
       "      <td>Eight to nine inches of snow fell across easte...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "0           201302         23        1900         201302       25       400   \n",
       "1           201312         14        2100         201312       15      1300   \n",
       "\n",
       "           STATE  STATE_FIPS  YEAR MONTH_NAME  ... END_LAT END_LON  \\\n",
       "0  NEW HAMPSHIRE          33  2013   February  ...     NaN     NaN   \n",
       "1  NEW HAMPSHIRE          33  2013   December  ...     NaN     NaN   \n",
       "\n",
       "                                   EPISODE_NARRATIVE  \\\n",
       "0  A coastal low moved southeast of southern New ...   \n",
       "1  Low pressure moved out of the midwest, off the...   \n",
       "\n",
       "                                     EVENT_NARRATIVE Y_PROP Y_CROP  \\\n",
       "0  Three to five inches of snow fell across easte...    0.0    0.0   \n",
       "1  Eight to nine inches of snow fell across easte...    0.0    0.0   \n",
       "\n",
       "  DURATION_HOURS LAT_MEAN  LON_MEAN  EVENT_SPAN_KM  \n",
       "0           33.0      NaN       NaN            0.0  \n",
       "1           16.0      NaN       NaN            0.0  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# === 2) Load & Clean ===\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your CSV path here or via env var STORM_CSV\n",
    "CSV_PATH = os.environ.get(\"STORM_CSV\", \"./StormEvents_details-ftp_v1.0_d2013_c20250520.csv\")\n",
    "\n",
    "def parse_damage(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v).strip().upper()\n",
    "    if not s: return np.nan\n",
    "    mult = 1\n",
    "    if s.endswith('K'): mult, s = 1_000, s[:-1]\n",
    "    elif s.endswith('M'): mult, s = 1_000_000, s[:-1]\n",
    "    elif s.endswith('B'): mult, s = 1_000_000_000, s[:-1]\n",
    "    try: return float(s) * mult\n",
    "    except:\n",
    "        try: return float(s.replace(',',''))\n",
    "        except: return np.nan\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False, encoding='utf-8')\n",
    "df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "assert 'DAMAGE_PROPERTY' in df.columns and 'DAMAGE_CROPS' in df.columns, \"CSV missing target columns\"\n",
    "\n",
    "df['Y_PROP'] = df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "df['Y_CROP'] = df['DAMAGE_CROPS'].apply(parse_damage)\n",
    "df = df[(df['Y_PROP'].notna()) | (df['Y_CROP'].notna())].copy()\n",
    "\n",
    "# Dates & duration\n",
    "for c in ['BEGIN_DATE_TIME','END_DATE_TIME']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce', infer_datetime_format=True)\n",
    "df['DURATION_HOURS'] = (df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']).dt.total_seconds()/3600\n",
    "df['DURATION_HOURS'] = df['DURATION_HOURS'].clip(lower=0).fillna(0)\n",
    "\n",
    "# Geospatial numeric\n",
    "for c in ['BEGIN_LAT','BEGIN_LON','END_LAT','END_LON']:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df['LAT_MEAN'] = df[['BEGIN_LAT','END_LAT']].mean(axis=1)\n",
    "df['LON_MEAN'] = df[['BEGIN_LON','END_LON']].mean(axis=1)\n",
    "\n",
    "# Robust haversine distance (km) between begin and end points (fallback to 0 if missing)\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    lat1 = np.deg2rad(lat1); lon1 = np.deg2rad(lon1)\n",
    "    lat2 = np.deg2rad(lat2); lon2 = np.deg2rad(lon2)\n",
    "    dlat = lat2 - lat1; dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return 6371.0 * c\n",
    "\n",
    "df['EVENT_SPAN_KM'] = haversine_km(df['BEGIN_LAT'].fillna(df['LAT_MEAN']),\n",
    "                                   df['BEGIN_LON'].fillna(df['LON_MEAN']),\n",
    "                                   df['END_LAT'].fillna(df['LAT_MEAN']),\n",
    "                                   df['END_LON'].fillna(df['LON_MEAN']))\n",
    "df['EVENT_SPAN_KM'] = df['EVENT_SPAN_KM'].fillna(0).astype('float32')\n",
    "\n",
    "# Drop high-cardinality / unused\n",
    "for c in ['EPISODE_ID','EVENT_ID','DATA_SOURCE','BEGIN_RANGE','END_RANGE','BEGIN_AZIMUTH','END_AZIMUTH']:\n",
    "    if c in df.columns: df.drop(columns=c, inplace=True)\n",
    "\n",
    "print(\"Cleaned shape:\", df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dd8273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 36581 7839 7839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3) Feature Lists & Base Matrix ===\n",
    "text_cols = [c for c in ['EPISODE_NARRATIVE','EVENT_NARRATIVE'] if c in df.columns]\n",
    "cat_cols  = [c for c in ['STATE','EVENT_TYPE','CZ_TYPE','CZ_NAME','LOCATION_NAME'] if c in df.columns]\n",
    "num_cols  = [c for c in ['INJURIES_DIRECT','INJURIES_INDIRECT','DEATHS_DIRECT','DEATHS_INDIRECT',\n",
    "                         'DURATION_HOURS','LAT_MEAN','LON_MEAN','EVENT_SPAN_KM'] if c in df.columns]\n",
    "\n",
    "X_cols = num_cols + cat_cols + text_cols\n",
    "df_model = df[X_cols + ['Y_PROP','Y_CROP','BEGIN_DATE_TIME']].copy()\n",
    "\n",
    "# Targets: natural and log\n",
    "y = np.column_stack([df_model['Y_PROP'].fillna(0).values, df_model['Y_CROP'].fillna(0).values])\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_model[X_cols].copy()\n",
    "X_train, X_temp, y_train, y_temp, y_train_log, y_temp_log = train_test_split(\n",
    "    X, y, y_log, test_size=0.30, random_state=SEED\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test, y_valid_log, y_test_log = train_test_split(\n",
    "    X_temp, y_temp, y_temp_log, test_size=0.50, random_state=SEED\n",
    ")\n",
    "print('Split sizes:', len(X_train), len(X_valid), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c355a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Building spatiotemporal network features (fast)‚Ä¶\n",
      "‚úÖ Added network features. New split sizes: 36581 7839 7839\n"
     ]
    }
   ],
   "source": [
    "# === 3b) Spatiotemporal Network Features (FAST) ‚Äî patched ===\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "print('üîß Building spatiotemporal network features (fast)‚Ä¶')\n",
    "\n",
    "df_net = pd.DataFrame(index=df_model.index)\n",
    "df_net['BEGIN_DATE_TIME'] = df_model['BEGIN_DATE_TIME']\n",
    "df_net['Y_PROP'] = df_model['Y_PROP']\n",
    "df_net['Y_CROP'] = df_model['Y_CROP']\n",
    "df_net['LAT_MEAN'] = df['LAT_MEAN'].values\n",
    "df_net['LON_MEAN'] = df['LON_MEAN'].values\n",
    "\n",
    "mask_valid = df_net['LAT_MEAN'].notna() & df_net['LON_MEAN'].notna()\n",
    "coords = np.vstack([\n",
    "    np.deg2rad(df_net.loc[mask_valid,'LAT_MEAN'].astype(float)),\n",
    "    np.deg2rad(df_net.loc[mask_valid,'LON_MEAN'].astype(float))\n",
    "]).T\n",
    "\n",
    "nn_within_radius = np.zeros(len(df_net), dtype=np.float32)\n",
    "recent_nn_6h = np.zeros(len(df_net), dtype=np.float32)\n",
    "recent_nn_24h = np.zeros(len(df_net), dtype=np.float32)\n",
    "recent_nn_72h = np.zeros(len(df_net), dtype=np.float32)\n",
    "mean_damage_nearby = np.zeros(len(df_net), dtype=np.float32)\n",
    "mean_damage_recent = np.zeros(len(df_net), dtype=np.float32)\n",
    "\n",
    "if len(coords) > 0:\n",
    "    tree = KDTree(coords, metric=\"euclidean\")\n",
    "    def km_to_rad(km): return km / 6371.0\n",
    "    RADIUS_RAD = km_to_rad(100)\n",
    "\n",
    "    dt = pd.to_datetime(df_net[\"BEGIN_DATE_TIME\"], errors=\"coerce\")\n",
    "    event_hours = (dt.astype('int64') // 1_000_000_000) / 3600.0  # minor: avoid .view warning\n",
    "    event_hours = event_hours.fillna(0).values.astype('float64')\n",
    "\n",
    "    valid_idx = np.flatnonzero(mask_valid.values)\n",
    "    for pos, i in enumerate(valid_idx):\n",
    "        idx = tree.query_radius(coords[pos:pos+1], r=RADIUS_RAD)[0]\n",
    "        neigh = valid_idx[idx]          # positional indices into df_net\n",
    "        neigh = neigh[neigh != i]\n",
    "        nn_within_radius[i] = len(neigh)\n",
    "        if len(neigh) > 0:\n",
    "            # nearby mean damage (use iloc with positional indices)  # FIX\n",
    "            mean_damage_nearby[i] = np.nanmean(\n",
    "                df_net.iloc[neigh][['Y_PROP','Y_CROP']].to_numpy()\n",
    "            )\n",
    "            # time filters\n",
    "            td = event_hours[i] - event_hours[neigh]\n",
    "            recent_nn_6h[i] = np.sum((0 < td) & (td <= 6))\n",
    "            recent_nn_24h[i] = np.sum((0 < td) & (td <= 24))\n",
    "            recent_nn_72h[i] = np.sum((0 < td) & (td <= 72))\n",
    "            mask_r = (0 < td) & (td <= 72)\n",
    "            if mask_r.any():\n",
    "                # recent mean damage (use iloc)                       # FIX\n",
    "                mean_damage_recent[i] = np.nanmean(\n",
    "                    df_net.iloc[neigh[mask_r]][['Y_PROP','Y_CROP']].to_numpy()\n",
    "                )\n",
    "\n",
    "# Attach\n",
    "extra_cols = ['NN_100KM','NN_6H','NN_24H','NN_72H','MEAN_DAMAGE_NEAR','MEAN_DAMAGE_RECENT']\n",
    "df_model['NN_100KM'] = nn_within_radius\n",
    "df_model['NN_6H'] = recent_nn_6h\n",
    "df_model['NN_24H'] = recent_nn_24h\n",
    "df_model['NN_72H'] = recent_nn_72h\n",
    "df_model['MEAN_DAMAGE_NEAR'] = mean_damage_nearby\n",
    "df_model['MEAN_DAMAGE_RECENT'] = mean_damage_recent\n",
    "\n",
    "# Rebuild X with dedup protection\n",
    "X = df_model[X_cols + extra_cols]\n",
    "X = X.loc[:, ~X.columns.duplicated()]\n",
    "for c in extra_cols:\n",
    "    if c not in num_cols: num_cols.append(c)\n",
    "\n",
    "# Re-split to align\n",
    "X_train, X_temp, y_train, y_temp, y_train_log, y_temp_log = train_test_split(\n",
    "    X, y, y_log, test_size=0.30, random_state=SEED\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test, y_valid_log, y_test_log = train_test_split(\n",
    "    X_temp, y_temp, y_temp_log, test_size=0.50, random_state=SEED\n",
    ")\n",
    "print('‚úÖ Added network features. New split sizes:', len(X_train), len(X_valid), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 4) Preprocessing (GPU‚Äësafe embeddings + memory‚Äësafe OHE) ===\n",
    "import os\n",
    "os.environ.setdefault('TRANSFORMERS_NO_TORCHVISION','1')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "EMBED_MODEL_NAME = os.getenv(\"EMBED_MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=(\"cuda\" if HAS_CUDA else \"cpu\"))\n",
    "try:\n",
    "    embed_model.max_seq_length = 256\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def embed_text_batched(X, batch_size=512):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        seq = X.iloc[:,0].astype(str).fillna('').tolist()\n",
    "    elif isinstance(X, pd.Series):\n",
    "        seq = X.astype(str).fillna('').tolist()\n",
    "    else:\n",
    "        seq = [str(t) for t in X]\n",
    "    outputs=[]; i=0; bs=batch_size\n",
    "    while i < len(seq):\n",
    "        j = min(i+bs, len(seq)); chunk = seq[i:j]\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                embs = embed_model.encode(chunk, batch_size=bs, convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=False)\n",
    "            outputs.append(embs.astype('float32', copy=False)); i=j\n",
    "        except RuntimeError as e:\n",
    "            if 'CUDA out of memory' in str(e) and bs>8 and HAS_CUDA:\n",
    "                torch.cuda.empty_cache(); bs = max(8, bs//2)\n",
    "                print(f\"[embed_text] OOM ‚Üí reducing batch_size to {bs}\")\n",
    "            else:\n",
    "                raise\n",
    "    if not outputs:\n",
    "        dim = embed_model.get_sentence_embedding_dimension()\n",
    "        return np.empty((0, dim), dtype=np.float32)\n",
    "    return np.vstack(outputs)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Handle sklearn version diff for OHE\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32, max_categories=200)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=np.float32)\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', ohe)\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if num_cols: transformers.append(('num', num_pipe, num_cols))\n",
    "if cat_cols: transformers.append(('cat', cat_pipe, cat_cols))\n",
    "for c in text_cols:\n",
    "    transformers.append((f'text_{c}', FunctionTransformer(embed_text_batched, validate=False), [c]))\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=transformers, remainder='drop', n_jobs=1)\n",
    "print(f'‚úÖ Preprocess ready. Text model: {EMBED_MODEL_NAME} on {(\"cuda\" if HAS_CUDA else \"cpu\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 5) XGB Tweedie (GPU‚Äëaware) + Slim Optuna ===\n",
    "import xgboost as xgb, optuna\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def sanitize_targets_nat(y_arr):\n",
    "    y_arr = np.nan_to_num(y_arr, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    y_arr[y_arr < 0] = 0.0\n",
    "    return y_arr\n",
    "\n",
    "def make_xgb_tweedie(base_params=None):\n",
    "    params = dict(objective='reg:tweedie', tweedie_variance_power=1.4,\n",
    "                  tree_method='hist', device=('cuda' if HAS_CUDA else 'cpu'),\n",
    "                  random_state=SEED, n_jobs=-1, max_bin=256,\n",
    "                  n_estimators=600 if MODE=='full' else 350,\n",
    "                  max_depth=6, learning_rate=0.05,\n",
    "                  subsample=0.9, colsample_bytree=0.8, reg_lambda=1.0)\n",
    "    if base_params: params.update(base_params)\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "def objective(trial, Xtr, ytr, Xva, yva):\n",
    "    params = {'n_estimators': trial.suggest_int('n_estimators', 250, 900),\n",
    "              'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "              'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.15, log=True),\n",
    "              'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "              'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 3.0, log=True),\n",
    "              'tweedie_variance_power': trial.suggest_float('tweedie_variance_power', 1.1, 1.9),\n",
    "              'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "              'tree_method': 'hist', 'device': ('cuda' if HAS_CUDA else 'cpu'),\n",
    "              'objective': 'reg:tweedie', 'random_state': SEED, 'n_jobs': -1}\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False)\n",
    "    pred = model.predict(Xva)\n",
    "    return float(mean_absolute_error(yva, pred))\n",
    "\n",
    "def run_optuna(name, Xtr, ytr, Xva, yva, n_trials):\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(lambda t: objective(t, Xtr, ytr, Xva, yva), n_trials=n_trials, show_progress_bar=False)\n",
    "    save_json(study.best_params, OUTDIR/f'optuna_tweedie_{name.lower()}.json')\n",
    "    return study.best_params\n",
    "\n",
    "with timer('Precompute preprocess for Tweedie'):\n",
    "    Xt_train = preprocess.fit_transform(X_train, y_train_log).astype('float32')\n",
    "    Xt_valid = preprocess.transform(X_valid).astype('float32')\n",
    "\n",
    "y_prop_tr = sanitize_targets_nat(y_train[:,0]); y_prop_va = sanitize_targets_nat(y_valid[:,0])\n",
    "y_crop_tr = sanitize_targets_nat(y_train[:,1]); y_crop_va = sanitize_targets_nat(y_valid[:,1])\n",
    "\n",
    "TRIALS = 12 if MODE in ('2h','3h') else 30\n",
    "best_prop = run_optuna('Property', Xt_train, y_prop_tr, Xt_valid, y_prop_va, n_trials=TRIALS)\n",
    "best_crop = run_optuna('Crop', Xt_train, y_crop_tr, Xt_valid, y_crop_va, n_trials=TRIALS)\n",
    "\n",
    "xgb_prop = make_xgb_tweedie(best_prop); xgb_prop.fit(Xt_train, y_prop_tr, eval_set=[(Xt_valid, y_prop_va)], verbose=False)\n",
    "xgb_crop = make_xgb_tweedie(best_crop); xgb_crop.fit(Xt_train, y_crop_tr, eval_set=[(Xt_valid, y_crop_va)], verbose=False)\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    return {'MAE': float(mean_absolute_error(y_true, y_pred)),\n",
    "            'RMSE': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "            'R2': float(r2_score(y_true, y_pred))}\n",
    "\n",
    "metrics = {'xgb_prop': eval_metrics(y_prop_va, xgb_prop.predict(Xt_valid)),\n",
    "           'xgb_crop': eval_metrics(y_crop_va, xgb_crop.predict(Xt_valid))}\n",
    "save_json(metrics, OUTDIR/'metrics_xgb_tweedie.json')\n",
    "save_joblib(xgb_prop, OUTDIR/'xgb_tweedie_property.joblib')\n",
    "save_joblib(xgb_crop, OUTDIR/'xgb_tweedie_crop.joblib')\n",
    "\n",
    "print('‚úÖ Saved tuned XGB Tweedie models and metrics to results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6) Baseline Ridge & XGB (context, fast) ===\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def metrics_frame(y_true, y_pred, labels):\n",
    "    out = {}\n",
    "    for i, name in enumerate(labels):\n",
    "        out[name] = dict(\n",
    "            MAE=float(mean_absolute_error(y_true[:, i], y_pred[:, i])),\n",
    "            RMSE=float(np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i]))),\n",
    "            R2=float(r2_score(y_true[:, i], y_pred[:, i])),\n",
    "        )\n",
    "    return pd.DataFrame(out).T\n",
    "\n",
    "# Precompute features for multi-target models\n",
    "with timer(\"Precompute preprocess on train/valid for baselines\"):\n",
    "    Xt_train_mt = Xt_train\n",
    "    Xt_valid_mt = Xt_valid\n",
    "\n",
    "def make_ridge(alpha=3.0): \n",
    "    return Ridge(alpha=alpha, fit_intercept=True, random_state=SEED)\n",
    "\n",
    "def make_xgb_fast():\n",
    "    return xgb.XGBRegressor(\n",
    "        n_estimators=350 if MODE!='full' else 700,\n",
    "        max_depth=6, learning_rate=0.07 if MODE!='full' else 0.05,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=SEED,\n",
    "        tree_method='hist', device=('cuda' if HAS_CUDA else 'cpu'), n_jobs=-1\n",
    "    )\n",
    "\n",
    "# Fit 2 independent targets for each baseline (simple approach)\n",
    "def fit_two_targets(builder):\n",
    "    m_prop = builder(); m_crop = builder()\n",
    "    m_prop.fit(Xt_train_mt, y_train[:,0]); m_crop.fit(Xt_train_mt, y_train[:,1])\n",
    "    p_prop = m_prop.predict(Xt_valid_mt); p_crop = m_crop.predict(Xt_valid_mt)\n",
    "    Yp = np.column_stack([p_prop, p_crop])\n",
    "    return Yp\n",
    "\n",
    "ridge_pred = fit_two_targets(make_ridge)\n",
    "xgb_pred = fit_two_targets(make_xgb_fast)\n",
    "\n",
    "mf_ridge = metrics_frame(y_valid, ridge_pred, ['damage_property','damage_crops'])\n",
    "mf_xgb = metrics_frame(y_valid, xgb_pred, ['damage_property','damage_crops'])\n",
    "\n",
    "save_json({'ridge': mf_ridge.to_dict(orient='index'),\n",
    "           'xgb'  : mf_xgb.to_dict(orient='index')},\n",
    "          OUTDIR/'validation_metrics_first_pass.json')\n",
    "\n",
    "print(\"\\nüìä Baseline (validation) results:\")\n",
    "print(\"Ridge:\\n\", mf_ridge)\n",
    "print(\"XGB  :\\n\", mf_xgb)\n",
    "print(\"‚úÖ Saved baseline metrics to results/validation_metrics_first_pass.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 7) Quantile XGB (Intervals) ===\n",
    "import xgboost as xgb\n",
    "\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "def make_quantile_xgb(q):\n",
    "    def quantile_obj(preds, dtrain):\n",
    "        y = dtrain.get_label()\n",
    "        e = y - preds\n",
    "        grad = np.where(e>0, -q, 1-q)\n",
    "        hess = np.ones_like(grad) * 1e-6\n",
    "        return grad, hess\n",
    "    def quantile_eval(preds, dtrain):\n",
    "        y = dtrain.get_label()\n",
    "        e = y - preds\n",
    "        loss = np.where(e>=0, q*e, (q-1)*e)\n",
    "        return 'q_pinball', float(np.mean(loss))\n",
    "    class XGBQuantile(xgb.XGBRegressor):\n",
    "        def fit(self, X, y):\n",
    "            dtrain = xgb.DMatrix(X, label=y)\n",
    "            self._Booster = xgb.train(\n",
    "                params={'max_depth': 6, 'eta': 0.05, 'subsample': 0.9, 'colsample_bytree': 0.9,\n",
    "                        'lambda': 1.0, 'tree_method': 'hist', 'verbosity': 0},\n",
    "                dtrain=dtrain, num_boost_round=(600 if MODE=='full' else 400),\n",
    "                obj=quantile_obj, feval=quantile_eval, verbose_eval=False\n",
    "            ); return self\n",
    "        def predict(self, X): return self._Booster.predict(xgb.DMatrix(X))\n",
    "    return XGBQuantile()\n",
    "\n",
    "if 'Xt_test' not in globals():\n",
    "    Xt_test = preprocess.transform(X_test).astype(np.float32, copy=False)\n",
    "\n",
    "# Use log targets for quantile on log-scale, then expm1\n",
    "ytr_p = y_train_log[:,0]; yva_p = y_valid_log[:,0]\n",
    "ytr_c = y_train_log[:,1]; yva_c = y_valid_log[:,1]\n",
    "\n",
    "quantile_models_xgb = {'prop':{}, 'crop':{}}\n",
    "preds_valid_q = {'prop':{}, 'crop':{}}\n",
    "for q in QUANTILES:\n",
    "    for target, (ytr, yva) in {'prop': (ytr_p, yva_p),\n",
    "                               'crop': (ytr_c, yva_c)}.items():\n",
    "        model = make_quantile_xgb(q); model.fit(Xt_train, ytr)\n",
    "        preds_valid_q[target][q] = model.predict(Xt_valid)\n",
    "        quantile_models_xgb[target][q] = model\n",
    "\n",
    "for q in QUANTILES:\n",
    "    for target, yv_log in {'prop': yva_p, 'crop': yva_c}.items():\n",
    "        pv = preds_valid_q[target][q]\n",
    "        diff = np.expm1(yv_log) - np.expm1(pv)\n",
    "        pin = float(np.mean(np.where(diff>=0, q*diff, (q-1)*diff)))\n",
    "        print(f\"q={q:.1f} {target}: pinball={pin:,.2f}\")\n",
    "\n",
    "preds_test = {}\n",
    "for target in ['prop','crop']:\n",
    "    preds_test[target] = {q: m.predict(Xt_test) for q,m in quantile_models_xgb[target].items()}\n",
    "\n",
    "interval_df = pd.DataFrame({\n",
    "    'prop_p10': np.expm1(preds_test['prop'][0.1]), 'prop_p50': np.expm1(preds_test['prop'][0.5]), 'prop_p90': np.expm1(preds_test['prop'][0.9]), 'prop_true': np.expm1(y_test_log[:,0]),\n",
    "    'crop_p10': np.expm1(preds_test['crop'][0.1]), 'crop_p50': np.expm1(preds_test['crop'][0.5]), 'crop_p90': np.expm1(preds_test['crop'][0.9]), 'crop_true': np.expm1(y_test_log[:,1]),\n",
    "})\n",
    "interval_df.to_csv(OUTDIR/'test_quantile_predictions.csv', index=False); print(\"‚úÖ Saved:\", OUTDIR/'test_quantile_predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 8) SHAP Analysis (decoded features) ===\n",
    "import shap, matplotlib.pyplot as plt\n",
    "\n",
    "# Manually construct feature names (numeric + cat OHE + text embeddings)\n",
    "feature_names = []\n",
    "\n",
    "for c in num_cols:\n",
    "    feature_names.append(f\"num__{c}\")\n",
    "for c in cat_cols:\n",
    "    feature_names.append(f\"cat__{c}_OHE\")  # TargetEncoder alt would be _te\n",
    "\n",
    "embed_dim = 384  # all-MiniLM-L6-v2\n",
    "try:\n",
    "    embed_dim = embed_model.get_sentence_embedding_dimension()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for c in text_cols:\n",
    "    for i in range(embed_dim):\n",
    "        feature_names.append(f\"text__{c}_emb_{i}\")\n",
    "\n",
    "np.save(OUTDIR/'feature_names.npy', np.array(feature_names, dtype=object))\n",
    "\n",
    "# Use tuned property model (as example) with a small sample for speed\n",
    "sample_n = min(2000, Xt_valid.shape[0])\n",
    "Xs = Xt_valid[:sample_n]\n",
    "explainer = shap.Explainer(xgb_prop)  # tree explainer\n",
    "sv = explainer(Xs, check_additivity=False)\n",
    "\n",
    "np.save(OUTDIR/'shap_values.npy', sv.values)\n",
    "np.save(OUTDIR/'shap_sample.npy', Xs)\n",
    "\n",
    "# Global bars\n",
    "plt.figure(); shap.plots.bar(sv, max_display=20, show=False); plt.tight_layout()\n",
    "plt.savefig(OUTDIR/'shap_bar.png', dpi=160, bbox_inches='tight'); plt.close()\n",
    "\n",
    "# Beeswarm\n",
    "plt.figure(); shap.plots.beeswarm(sv, max_display=20, show=False); plt.tight_layout()\n",
    "plt.savefig(OUTDIR/'shap_beeswarm.png', dpi=160, bbox_inches='tight'); plt.close()\n",
    "\n",
    "print(\"‚úÖ Saved SHAP artifacts to results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0252fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 9) Interval Diagnostics (coverage & reliability) ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_int = pd.read_csv(OUTDIR/'test_quantile_predictions.csv')\n",
    "def cov_rate(y, lo, hi): \n",
    "    return float(np.mean((y >= lo) & (y <= hi)))\n",
    "\n",
    "prop_cov = cov_rate(df_int['prop_true'], df_int['prop_p10'], df_int['prop_p90'])\n",
    "crop_cov = cov_rate(df_int['crop_true'], df_int['crop_p10'], df_int['crop_p90'])\n",
    "\n",
    "# Bar plot\n",
    "fig = plt.figure()\n",
    "plt.bar(['Property','Crops'], [prop_cov, crop_cov])\n",
    "plt.axhline(0.8, linestyle='--')\n",
    "plt.title('80% Interval Coverage (p10‚Äìp90)')\n",
    "plt.ylim(0,1); plt.tight_layout()\n",
    "plt.savefig(OUTDIR/'quantile_coverage_bar.png', dpi=160, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "# Reliability (quantile calibration diagnostic)\n",
    "qs = [0.1, 0.5, 0.9]\n",
    "def quantile_reliability(y_true, preds_dict):\n",
    "    pts = []\n",
    "    for q in qs:\n",
    "        yhat = preds_dict[q]\n",
    "        # fraction below predicted quantile ~ q if well calibrated\n",
    "        frac = float(np.mean(y_true <= yhat))\n",
    "        pts.append((q, frac))\n",
    "    return pts\n",
    "\n",
    "prop_pts = quantile_reliability(df_int['prop_true'], {q: df_int[f'prop_p{int(q*100)}'] for q in qs})\n",
    "crop_pts = quantile_reliability(df_int['crop_true'], {q: df_int[f'crop_p{int(q*100)}'] for q in qs})\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.scatter([p[0] for p in prop_pts], [p[1] for p in prop_pts], label='Property')\n",
    "plt.scatter([p[0] for p in crop_pts], [p[1] for p in crop_pts], label='Crops')\n",
    "plt.legend(); plt.xlabel('Nominal quantile'); plt.ylabel('Observed fraction'); plt.title('Quantile Reliability')\n",
    "plt.tight_layout(); plt.savefig(OUTDIR/'quantile_intervals_reliability.png', dpi=160, bbox_inches='tight'); plt.close(fig)\n",
    "\n",
    "print(f\"Coverage ‚Äî property: {prop_cov:.3f}, crops: {crop_cov:.3f}\")\n",
    "print('‚úÖ Saved interval diagnostics images to results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e17a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 10) Export predict.py helper (batch predictions) ===\n",
    "script = '''#!/usr/bin/env python3\n",
    "import sys, pandas as pd, numpy as np, joblib\n",
    "def main(inp,out):\n",
    "    bprop=joblib.load(\"results/xgb_tweedie_property.joblib\")\n",
    "    bcrop=joblib.load(\"results/xgb_tweedie_crop.joblib\")\n",
    "    pre=joblib.load(\"results/preprocess.joblib\")\n",
    "    df=pd.read_csv(inp)\n",
    "    X=pre.transform(df)\n",
    "    p_prop=bprop.predict(X); p_crop=bcrop.predict(X)\n",
    "    pd.DataFrame({\"pred_damage_property\":p_prop,\"pred_damage_crops\":p_crop}).to_csv(out,index=False)\n",
    "if __name__==\"__main__\": main(sys.argv[1], sys.argv[2])\n",
    "'''\n",
    "p = OUTDIR/'predict.py'\n",
    "with open(p,'w', encoding='utf-8') as f: f.write(script)\n",
    "print(\"‚úÖ Wrote\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 11) Save preprocess artifact ===\n",
    "import joblib\n",
    "joblib.dump(preprocess, OUTDIR/'preprocess.joblib')\n",
    "print(\"‚úÖ Saved:\", OUTDIR/'preprocess.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
