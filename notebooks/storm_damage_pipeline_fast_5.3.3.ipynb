{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bb02e6",
   "metadata": {},
   "source": [
    "# ðŸŒªï¸ Storm Damage Pipeline â€” **FAST 5.6** (GPUâ€‘aware, cached embeddings, slim tuning)\n",
    "\n",
    "Designed to finish quickly on midâ€‘range GPUs (e.g., RTX 3050 Ti). Tactics: cached MiniLM embeddings, sparse OHE, slim Optuna (6â€“10 trials), early stopping, reduced seq length.\n",
    "\n",
    "**Outputs:** `./results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267af516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 0) Setup & Utilities ===\n",
    "import os, sys, math, json, time, hashlib, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RESULTS = Path('./results'); RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 42\n",
    "MODE = os.getenv('STORM_MODE', 'fast')  # 'fast' | 'full'\n",
    "\n",
    "def timer(label):\n",
    "    class T:\n",
    "        def __enter__(self_s): self_s.t0=time.time(); print(f'â±ï¸ {label} ...'); return self_s\n",
    "        def __exit__(self_s, *exc): print(f'â±ï¸ {label}: {time.time()-self_s.t0:.2f}s')\n",
    "    return T()\n",
    "\n",
    "def save_json(obj, path): \n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w', encoding='utf-8') as f: json.dump(obj, f, indent=2)\n",
    "\n",
    "def save_joblib(obj, path):\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(obj, path)\n",
    "\n",
    "def has_cuda():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "HAS_CUDA = has_cuda()\n",
    "print('GPU available:', HAS_CUDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1) Load Data ===\n",
    "CSV_PATH = os.environ.get('STORM_CSV', './StormEvents_details-ftp_v1.0_d2013_c20250520.csv')\n",
    "print('CSV_PATH =', CSV_PATH)\n",
    "\n",
    "def parse_damage(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v).strip().upper()\n",
    "    if not s: return np.nan\n",
    "    mult = 1\n",
    "    if s.endswith('K'): mult, s = 1_000, s[:-1]\n",
    "    elif s.endswith('M'): mult, s = 1_000_000, s[:-1]\n",
    "    elif s.endswith('B'): mult, s = 1_000_000_000, s[:-1]\n",
    "    try: return float(s) * mult\n",
    "    except:\n",
    "        try: return float(s.replace(',',''))\n",
    "        except: return np.nan\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False, encoding='utf-8')\n",
    "df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "assert 'DAMAGE_PROPERTY' in df.columns and 'DAMAGE_CROPS' in df.columns, 'Targets not found.'\n",
    "\n",
    "df['Y_PROP'] = df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "df['Y_CROP'] = df['DAMAGE_CROPS'].apply(parse_damage)\n",
    "df = df[(df['Y_PROP'].notna()) | (df['Y_CROP'].notna())].copy()\n",
    "\n",
    "for c in ['BEGIN_DATE_TIME','END_DATE_TIME']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "df['DURATION_HOURS'] = (df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']).dt.total_seconds()/3600\n",
    "df['DURATION_HOURS'] = df['DURATION_HOURS'].clip(lower=0).fillna(0)\n",
    "\n",
    "for c in ['BEGIN_LAT','BEGIN_LON','END_LAT','END_LON']:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df['LAT_MEAN'] = df[['BEGIN_LAT','END_LAT']].mean(axis=1)\n",
    "df['LON_MEAN'] = df[['BEGIN_LON','END_LON']].mean(axis=1)\n",
    "df['LAT_SIN'] = np.sin(np.deg2rad(df['LAT_MEAN']))\n",
    "df['LAT_COS'] = np.cos(np.deg2rad(df['LAT_MEAN']))\n",
    "df['LON_SIN'] = np.sin(np.deg2rad(df['LON_MEAN']))\n",
    "df['LON_COS'] = np.cos(np.deg2rad(df['LON_MEAN']))\n",
    "\n",
    "for c in ['EPISODE_ID','EVENT_ID','DATA_SOURCE','BEGIN_RANGE','END_RANGE','BEGIN_AZIMUTH','END_AZIMUTH']:\n",
    "    if c in df.columns: df.drop(columns=c, inplace=True)\n",
    "\n",
    "print('Cleaned shape:', df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0abb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 2) Feature Schema & Split ===\n",
    "text_cols = [c for c in ['EVENT_NARRATIVE','EPISODE_NARRATIVE'] if c in df.columns]\n",
    "num_cols = [c for c in ['INJURIES_DIRECT','INJURIES_INDIRECT','DEATHS_DIRECT','DEATHS_INDIRECT',\n",
    "                        'DURATION_HOURS','LAT_MEAN','LON_MEAN','LAT_SIN','LAT_COS','LON_SIN','LON_COS']\n",
    "            if c in df.columns]\n",
    "cat_cols = [c for c in ['STATE','CZ_NAME','EVENT_TYPE'] if c in df.columns]\n",
    "\n",
    "y = df[['Y_PROP','Y_CROP']].to_numpy(dtype='float32')\n",
    "y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "X_tab = df[num_cols + cat_cols].copy()\n",
    "\n",
    "X_train_tab, X_temp_tab, y_train, y_temp = train_test_split(X_tab, y, test_size=0.30, random_state=SEED)\n",
    "X_valid_tab, X_test_tab, y_valid, y_test = train_test_split(X_temp_tab, y_temp, test_size=0.50, random_state=SEED)\n",
    "\n",
    "texts = {c: df[c].astype(str).fillna('') for c in text_cols}\n",
    "X_train_txt = {c: texts[c].loc[X_train_tab.index] for c in text_cols}\n",
    "X_valid_txt = {c: texts[c].loc[X_valid_tab.index] for c in text_cols}\n",
    "X_test_txt  = {c: texts[c].loc[X_test_tab.index]  for c in text_cols}\n",
    "\n",
    "print('Splits ->', 'train:', X_train_tab.shape, 'valid:', X_valid_tab.shape, 'test:',  X_test_tab.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77526366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 3) Cached MiniLM Embeddings ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch, hashlib, numpy as np\n",
    "\n",
    "EMBED_MODEL_NAME = os.getenv('EMBED_MODEL', 'all-MiniLM-L6-v2')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=DEVICE)\n",
    "try: embed_model.max_seq_length = 128\n",
    "except Exception: pass\n",
    "\n",
    "CACHE = Path('./results/embed_cache'); CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def md5_of_series(ser):\n",
    "    h = hashlib.md5()\n",
    "    for s in ser.astype(str).values:\n",
    "        h.update(s.encode('utf-8', errors='ignore')); h.update(b'\\n')\n",
    "    return h.hexdigest()\n",
    "\n",
    "def cache_embed(series):\n",
    "    series = series.astype(str).fillna('')\n",
    "    key = f\"{EMBED_MODEL_NAME}_{len(series)}_{md5_of_series(series)}_{embed_model.get_sentence_embedding_dimension()}_{embed_model.max_seq_length}_{DEVICE}.npy\"\n",
    "    path = CACHE / key\n",
    "    if path.exists():\n",
    "        return np.load(path)\n",
    "    data = series.tolist()\n",
    "    out = []\n",
    "    bs = 768 if DEVICE=='cuda' else 256\n",
    "    i=0\n",
    "    while i < len(data):\n",
    "        j = min(i+bs, len(data)); chunk = data[i:j]\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                em = embed_model.encode(chunk, batch_size=bs, convert_to_numpy=True,\n",
    "                                        show_progress_bar=False, normalize_embeddings=False)\n",
    "            out.append(em.astype('float32', copy=False)); i = j\n",
    "        except RuntimeError as e:\n",
    "            if 'CUDA out of memory' in str(e) and bs > 16 and DEVICE=='cuda':\n",
    "                torch.cuda.empty_cache(); bs //= 2; print('[embed] OOM â†’ batch_size =', bs)\n",
    "            else:\n",
    "                raise\n",
    "    arr = np.vstack(out) if out else np.empty((0, embed_model.get_sentence_embedding_dimension()), dtype='float32')\n",
    "    np.save(path, arr); return arr\n",
    "\n",
    "emb_train = []; emb_valid = []; emb_test = []\n",
    "for c in text_cols:\n",
    "    with timer(f'Embed {c}'):\n",
    "        emb_train.append(cache_embed(X_train_txt[c]))\n",
    "        emb_valid.append(cache_embed(X_valid_txt[c]))\n",
    "        emb_test.append(cache_embed(X_test_txt[c]))\n",
    "\n",
    "EMB_DIM = embed_model.get_sentence_embedding_dimension() if text_cols else 0\n",
    "print('Embedding dim:', EMB_DIM, '| #text cols:', len(text_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55986592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 4) Tabular Preprocessing + Concatenate Embeddings ===\n",
    "from scipy import sparse\n",
    "\n",
    "num_pipe = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                     ('scaler', StandardScaler())])\n",
    "\n",
    "cat_pipe = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                     ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=True, max_categories=100))])\n",
    "\n",
    "pre_tab = ColumnTransformer([\n",
    "    ('num', num_pipe, [c for c in X_train_tab.columns if c in num_cols]),\n",
    "    ('cat', cat_pipe, [c for c in X_train_tab.columns if c in cat_cols])\n",
    "], remainder='drop', sparse_threshold=0.3)\n",
    "\n",
    "with timer('Fit tabular preproc'):\n",
    "    Xt_train_tab = pre_tab.fit_transform(X_train_tab)\n",
    "    Xt_valid_tab = pre_tab.transform(X_valid_tab)\n",
    "    Xt_test_tab  = pre_tab.transform(X_test_tab)\n",
    "\n",
    "def hstack_features(tab, embs_list):\n",
    "    if not embs_list:\n",
    "        return tab if sparse.issparse(tab) else sparse.csr_matrix(tab)\n",
    "    tab_csr = tab if sparse.issparse(tab) else sparse.csr_matrix(tab)\n",
    "    embs = np.concatenate(embs_list, axis=1)\n",
    "    return sparse.hstack([tab_csr, sparse.csr_matrix(embs)], format='csr')\n",
    "\n",
    "Xt_train = hstack_features(Xt_train_tab, emb_train)\n",
    "Xt_valid = hstack_features(Xt_valid_tab, emb_valid)\n",
    "Xt_test  = hstack_features(Xt_test_tab,  emb_test)\n",
    "print('Xt shapes:', Xt_train.shape, Xt_valid.shape, Xt_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 5) XGB Tweedie (GPUâ€‘aware) + Slim Optuna ===\n",
    "import xgboost as xgb, optuna\n",
    "\n",
    "def sanitize_targets_nat(y_arr):\n",
    "    y_arr = np.nan_to_num(y_arr, nan=0.0, posinf=0.0, neginf=0.0).astype('float32')\n",
    "    y_arr[y_arr < 0] = 0.0\n",
    "    return y_arr\n",
    "\n",
    "y_prop_tr = sanitize_targets_nat(y_train[:,0]); y_prop_va = sanitize_targets_nat(y_valid[:,0]); y_prop_te = sanitize_targets_nat(y_test[:,0])\n",
    "y_crop_tr = sanitize_targets_nat(y_train[:,1]); y_crop_va = sanitize_targets_nat(y_valid[:,1]); y_crop_te = sanitize_targets_nat(y_test[:,1])\n",
    "\n",
    "def make_xgb_tweedie(base=None):\n",
    "    params = dict(objective='reg:tweedie', tweedie_variance_power=1.4,\n",
    "                  tree_method='hist', device=('cuda' if HAS_CUDA else 'cpu'),\n",
    "                  predictor=('gpu_predictor' if HAS_CUDA else 'auto'),\n",
    "                  random_state=SEED, n_jobs=-1, max_bin=256,\n",
    "                  n_estimators=(420 if MODE=='fast' else 700),\n",
    "                  max_depth=6, learning_rate=0.07 if MODE=='fast' else 0.05,\n",
    "                  subsample=0.9, colsample_bytree=0.8, reg_lambda=1.0)\n",
    "    if base: params.update(base)\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "def objective(trial, Xtr, ytr, Xva, yva):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 240, 600),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.12, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 3.0, log=True),\n",
    "        'tweedie_variance_power': trial.suggest_float('tweedie_variance_power', 1.1, 1.9),\n",
    "        'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "        'tree_method': 'hist',\n",
    "        'device': ('cuda' if HAS_CUDA else 'cpu'),\n",
    "        'predictor': ('gpu_predictor' if HAS_CUDA else 'auto'),\n",
    "        'objective': 'reg:tweedie',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(Xtr, ytr, eval_set=[(Xva, yva)], verbose=False, early_stopping_rounds=40)\n",
    "    pred = model.predict(Xva)\n",
    "    return float(mean_absolute_error(yva, pred))\n",
    "\n",
    "def run_optuna(name, Xtr, ytr, Xva, yva, n_trials):\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(lambda t: objective(t, Xtr, ytr, Xva, yva), n_trials=n_trials, show_progress_bar=False)\n",
    "    print(f'âœ… {name} best MAE: {study.best_value:.3f}')\n",
    "    save_json(study.best_params, RESULTS/f'optuna_tweedie_{name.lower()}.json')\n",
    "    return study.best_params\n",
    "\n",
    "TRIALS = 6 if MODE=='fast' else 20\n",
    "with timer('Optuna: Property'):\n",
    "    best_prop = run_optuna('Property', Xt_train, y_prop_tr, Xt_valid, y_prop_va, n_trials=TRIALS)\n",
    "with timer('Optuna: Crop'):\n",
    "    best_crop = run_optuna('Crop', Xt_train, y_crop_tr, Xt_valid, y_crop_va, n_trials=TRIALS)\n",
    "\n",
    "xgb_prop = make_xgb_tweedie(best_prop)\n",
    "xgb_crop = make_xgb_tweedie(best_crop)\n",
    "with timer('Fit final Property'):\n",
    "    xgb_prop.fit(Xt_train, y_prop_tr, eval_set=[(Xt_valid, y_prop_va)], verbose=False, early_stopping_rounds=50)\n",
    "with timer('Fit final Crop'):\n",
    "    xgb_crop.fit(Xt_train, y_crop_tr, eval_set=[(Xt_valid, y_crop_va)], verbose=False, early_stopping_rounds=50)\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    return {'MAE': float(mean_absolute_error(y_true, y_pred)),\n",
    "            'RMSE': float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "            'R2': float(r2_score(y_true, y_pred))}\n",
    "\n",
    "metrics = {\n",
    "    'valid_property': eval_metrics(y_prop_va, xgb_prop.predict(Xt_valid)),\n",
    "    'valid_crop':     eval_metrics(y_crop_va, xgb_crop.predict(Xt_valid)),\n",
    "    'test_property':  eval_metrics(y_prop_te, xgb_prop.predict(Xt_test)),\n",
    "    'test_crop':      eval_metrics(y_crop_te, xgb_crop.predict(Xt_test)),\n",
    "}\n",
    "save_json(metrics, RESULTS/'metrics_fast_tweedie.json')\n",
    "save_joblib(xgb_prop, RESULTS/'xgb_tweedie_property_fast.joblib')\n",
    "save_joblib(xgb_crop, RESULTS/'xgb_tweedie_crop_fast.joblib')\n",
    "print('âœ… Saved tuned XGB Tweedie models & metrics â†’', RESULTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 6) Optional SHAP (small sample) ===\n",
    "DO_SHAP = os.getenv('DO_SHAP','0') == '1'\n",
    "if DO_SHAP:\n",
    "    import shap, scipy.sparse as sp\n",
    "    nsamp = min(1500, Xt_valid.shape[0])\n",
    "    Xv = Xt_valid[:nsamp].toarray() if hasattr(Xt_valid, 'toarray') else Xt_valid[:nsamp]\n",
    "    explainer = shap.TreeExplainer(xgb_prop)\n",
    "    sv = explainer.shap_values(Xv)\n",
    "    np.save(RESULTS/'shap_values_property_fast.npy', sv)\n",
    "    print('âœ… Saved SHAP â†’', RESULTS/'shap_values_property_fast.npy')\n",
    "else:\n",
    "    print('â„¹ï¸ SHAP skipped. Set DO_SHAP=1 to enable.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
