{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d411b51a",
   "metadata": {},
   "source": [
    "\n",
    "# üå™Ô∏è Storm Damage Prediction ‚Äî v4.2 (GPU‚ÄëAccelerated)\n",
    "\n",
    "End‚Äëto‚Äëend pipeline to predict **Property** and **Crop** damages from NOAA Storm Events using:\n",
    "- Text + tabular features (SentenceTransformer embeddings + engineered features)\n",
    "- Fast **GPU XGBoost** training & prediction\n",
    "- **SHAP** (GPU TreeSHAP) for feature importance\n",
    "- Quantile intervals via residual bootstrapping\n",
    "\n",
    "**Outputs:** written to `./results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34f591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE=2h | HAS_CUDA=True | DEVICE=cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1) Setup & Config ===\n",
    "import os, math, json, gc, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Runtime mode: '2h', '5h', 'full'\n",
    "MODE = os.environ.get(\"PIPELINE_MODE\", \"2h\")\n",
    "\n",
    "# Paths\n",
    "OUTDIR = Path(\"./results\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU / CUDA\n",
    "import torch\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if HAS_CUDA else \"cpu\"\n",
    "print(f\"MODE={MODE} | HAS_CUDA={HAS_CUDA} | DEVICE={DEVICE}\")\n",
    "\n",
    "# SentenceTransformer model (384‚Äëdim, fast & light)\n",
    "EMBED_MODEL_NAME = os.environ.get(\"EMBED_MODEL_NAME\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Simple timer\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def timer(msg: str):\n",
    "    t0 = time.time()\n",
    "    print(f\"‚è±Ô∏è {msg} ...\", flush=True)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"‚è±Ô∏è {msg}: {dt:.2f}s\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d915c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Load CSV ...\n",
      "‚è±Ô∏è Load CSV: 0.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ducan\\AppData\\Local\\Temp\\ipykernel_9884\\3130023973.py:40: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ducan\\AppData\\Local\\Temp\\ipykernel_9884\\3130023973.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ducan\\AppData\\Local\\Temp\\ipykernel_9884\\3130023973.py:40: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
      "C:\\Users\\ducan\\AppData\\Local\\Temp\\ipykernel_9884\\3130023973.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned shape: (52259, 47)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2) Load & Clean ===\n",
    "# Set your CSV path here:\n",
    "CSV_PATH = os.environ.get(\"CSV_PATH\", \"StormEvents_details-ftp_v1.0_d2013_c20250520.csv\")\n",
    "\n",
    "def parse_damage(v):\n",
    "    if pd.isna(v): return np.nan\n",
    "    s = str(v).strip().upper()\n",
    "    if not s: return np.nan\n",
    "    mult = 1\n",
    "    if s.endswith('K'): mult, s = 1_000, s[:-1]\n",
    "    elif s.endswith('M'): mult, s = 1_000_000, s[:-1]\n",
    "    elif s.endswith('B'): mult, s = 1_000_000_000, s[:-1]\n",
    "    try:\n",
    "        return float(s) * mult\n",
    "    except:\n",
    "        try:\n",
    "            return float(s.replace(',',''))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "with timer(\"Load CSV\"):\n",
    "    df = pd.read_csv(CSV_PATH, low_memory=False, encoding='utf-8')\n",
    "\n",
    "df.columns = [c.strip().upper() for c in df.columns]\n",
    "\n",
    "# Basic drop of IDs we won't use\n",
    "for dropc in ['EPISODE_ID','EVENT_ID','DATA_SOURCE']:\n",
    "    if dropc in df.columns:\n",
    "        df.drop(columns=dropc, inplace=True)\n",
    "\n",
    "assert 'DAMAGE_PROPERTY' in df.columns and 'DAMAGE_CROPS' in df.columns, \"CSV missing DAMAGE_PROPERTY / DAMAGE_CROPS\"\n",
    "\n",
    "# Targets\n",
    "df['Y_PROP'] = df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "df['Y_CROP'] = df['DAMAGE_CROPS'].apply(parse_damage)\n",
    "df = df[(df['Y_PROP'].notna()) | (df['Y_CROP'].notna())].copy()\n",
    "\n",
    "# Dates\n",
    "def to_dt(series):\n",
    "    return pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n",
    "for c in ['BEGIN_DATE_TIME','END_DATE_TIME']:\n",
    "    if c in df.columns: df[c] = to_dt(df[c])\n",
    "\n",
    "df['DURATION_HOURS'] = (df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']).dt.total_seconds()/3600\n",
    "df['DURATION_HOURS'] = df['DURATION_HOURS'].clip(lower=0).fillna(0)\n",
    "\n",
    "# Geo to numeric\n",
    "for c in ['BEGIN_LAT','BEGIN_LON','END_LAT','END_LON']:\n",
    "    if c in df.columns: df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df['LAT_MEAN'] = df[['BEGIN_LAT','END_LAT']].mean(axis=1)\n",
    "df['LON_MEAN'] = df[['BEGIN_LON','END_LON']].mean(axis=1)\n",
    "\n",
    "# Trig features\n",
    "df['LAT_SIN'] = np.sin(np.deg2rad(df['LAT_MEAN']))\n",
    "df['LAT_COS'] = np.cos(np.deg2rad(df['LAT_MEAN']))\n",
    "df['LON_SIN'] = np.sin(np.deg2rad(df['LON_MEAN']))\n",
    "df['LON_COS'] = np.cos(np.deg2rad(df['LON_MEAN']))\n",
    "\n",
    "# Drop some detailed geo/location if present\n",
    "for c in ['BEGIN_LAT','BEGIN_LON','END_LAT','END_LON','BEGIN_LOCATION','END_LOCATION','BEGIN_AZIMUTH','END_AZIMUTH','BEGIN_RANGE','END_RANGE']:\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=c, inplace=True)\n",
    "\n",
    "print(\"Cleaned shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67409a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 36581 7839 7839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3) Feature Lists & Split ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_cols = [c for c in ['EPISODE_NARRATIVE','EVENT_NARRATIVE'] if c in df.columns]\n",
    "cat_cols  = [c for c in ['STATE','EVENT_TYPE','CZ_TYPE','CZ_NAME','LOCATION_NAME'] if c in df.columns]\n",
    "num_cols  = [c for c in [\n",
    "    'INJURIES_DIRECT','INJURIES_INDIRECT','DEATHS_DIRECT','DEATHS_INDIRECT','DURATION_HOURS',\n",
    "    'LAT_MEAN','LON_MEAN','LAT_SIN','LAT_COS','LON_SIN','LON_COS'\n",
    "] if c in df.columns]\n",
    "\n",
    "X_cols = num_cols + cat_cols + text_cols\n",
    "df_model = df[X_cols + ['Y_PROP','Y_CROP']].copy()\n",
    "\n",
    "y = np.column_stack([df_model['Y_PROP'].fillna(0).values, df_model['Y_CROP'].fillna(0).values])\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X = df_model[X_cols].copy()\n",
    "X_train, X_temp, y_train_log, y_temp_log = train_test_split(X, y_log, test_size=0.30, random_state=42)\n",
    "X_valid, X_test, y_valid_log, y_test_log = train_test_split(X_temp, y_temp_log, test_size=0.50, random_state=42)\n",
    "\n",
    "print('Split sizes:', len(X_train), len(X_valid), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031023fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocess ready. Text model: sentence-transformers/all-MiniLM-L6-v2 on cuda\n"
     ]
    }
   ],
   "source": [
    "# === 4) Preprocessing (GPU-safe embeddings + memory-safe OHE) ===\n",
    "import os\n",
    "os.environ.setdefault('TRANSFORMERS_NO_TORCHVISION','1')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "embed_model = SentenceTransformer(\n",
    "    EMBED_MODEL_NAME,\n",
    "    device=(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "try:\n",
    "    embed_model.max_seq_length = 256\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def embed_text_batched(X, batch_size=512):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        seq = X.iloc[:, 0].astype(str).fillna('').tolist()\n",
    "    elif isinstance(X, pd.Series):\n",
    "        seq = X.astype(str).fillna('').tolist()\n",
    "    else:\n",
    "        seq = [str(t) for t in X]\n",
    "\n",
    "    outputs = []\n",
    "    i = 0\n",
    "    bs = batch_size\n",
    "\n",
    "    while i < len(seq):\n",
    "        j = min(i + bs, len(seq))\n",
    "        chunk = seq[i:j]\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                embs = embed_model.encode(\n",
    "                    chunk,\n",
    "                    batch_size=bs,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False,\n",
    "                    normalize_embeddings=False\n",
    "                )\n",
    "            outputs.append(embs.astype('float32', copy=False))\n",
    "            i = j\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'CUDA out of memory' in str(e) and bs > 8 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                bs = max(8, bs // 2)\n",
    "                print(f\"[embed_text] OOM ‚Üí reducing batch_size to {bs}\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    if not outputs:\n",
    "        dim = embed_model.get_sentence_embedding_dimension()\n",
    "        return np.empty((0, dim), dtype=np.float32)\n",
    "\n",
    "    return np.vstack(outputs)\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ‚úÖ Categorical pipeline (memory-safe OHE)\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(\n",
    "        handle_unknown='ignore',\n",
    "        sparse_output=False,     # ‚úÖ fixed argument\n",
    "        dtype=np.float32,\n",
    "        max_categories=200       # ‚úÖ memory-safe cap\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Build column transformer\n",
    "transformers = []\n",
    "if num_cols:\n",
    "    transformers.append(('num', num_pipe, num_cols))\n",
    "if cat_cols:\n",
    "    transformers.append(('cat', cat_pipe, cat_cols))\n",
    "for c in text_cols:\n",
    "    transformers.append((f'text_{c}', FunctionTransformer(embed_text_batched, validate=False), [c]))\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='drop',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocess ready. Text model: {EMBED_MODEL_NAME} on {('cuda' if torch.cuda.is_available() else 'cpu')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27db5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: ['INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT', 'DURATION_HOURS', 'LAT_MEAN', 'LON_MEAN', 'LAT_SIN'] ...\n",
      "Categorical: ['STATE', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_NAME'] \n",
      "Text: ['EPISODE_NARRATIVE', 'EVENT_NARRATIVE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 5) Sanity Check ===\n",
    "print(\"Numeric:\", num_cols[:8], \"...\" if len(num_cols)>8 else \"\")\n",
    "print(\"Categorical:\", cat_cols[:8], \"...\" if len(cat_cols)>8 else \"\")\n",
    "print(\"Text:\", text_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9cfff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Step 6 ‚Äî Optimized GPU-Accelerated Evaluation\n",
      "‚è±Ô∏è Precompute preprocess on train/valid (GPU embeddings) ...\n",
      "‚è±Ô∏è Precompute preprocess on train/valid (GPU embeddings): 368.72s\n",
      "‚úÖ Saved feature_names.npy\n",
      "üöÄ Running first‚Äëpass‚Ä¶\n",
      "‚ö° XGB: Using GPU\n",
      "‚è±Ô∏è fit ridge (validation) ...\n",
      "‚è±Ô∏è fit ridge (validation): 0.34s\n",
      "\n",
      "üìä ridge (validation) results:\n",
      "                          MAE          RMSE           R2\n",
      "damage_property  2.084303e+06  1.734214e+08 -4040.446716\n",
      "damage_crops     1.098022e+05  2.624504e+06    -0.001658\n",
      "\n",
      "‚è±Ô∏è fit xgb (validation) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:36:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:36:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è fit xgb (validation): 56.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:37:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:37:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä xgb (validation) results:\n",
      "                           MAE          RMSE        R2\n",
      "damage_property  123648.468662  2.724689e+06  0.002379\n",
      "damage_crops      56287.413170  2.375631e+06  0.179303\n",
      "\n",
      "‚úÖ Saved first-pass metrics to results\\validation_metrics_first_pass.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 6) First-Pass Model Evaluation (GPU-accelerated) ===\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"üîß Step 6 ‚Äî Optimized GPU-Accelerated Evaluation\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with timer(\"Precompute preprocess on train/valid (GPU embeddings)\"):\n",
    "    Xt_train = preprocess.fit_transform(X_train, y_train_log).astype(\"float32\")\n",
    "    Xt_valid = preprocess.transform(X_valid).astype(\"float32\")\n",
    "\n",
    "feature_names = []\n",
    "for c in num_cols: feature_names.append(f\"num__{c}\")\n",
    "for c in cat_cols: feature_names.append(f\"cat__{c}_oh\")\n",
    "embed_dim = 384  # all-MiniLM-L6-v2\n",
    "try:\n",
    "    embed_dim = embed_model.get_sentence_embedding_dimension()\n",
    "except Exception:\n",
    "    pass\n",
    "for c in text_cols:\n",
    "    feature_names.extend([f\"text__{c}_emb_{i}\" for i in range(embed_dim)])\n",
    "np.save(\"feature_names.npy\", np.array(feature_names, dtype=object))\n",
    "print(\"‚úÖ Saved feature_names.npy\")\n",
    "\n",
    "def metrics_frame(y_true, y_pred, labels):\n",
    "    out={}\n",
    "    for i,name in enumerate(labels):\n",
    "        out[name]=dict(\n",
    "            MAE=float(mean_absolute_error(y_true[:,i], y_pred[:,i])),\n",
    "            RMSE=float(math.sqrt(mean_squared_error(y_true[:,i], y_pred[:,i]))),\n",
    "            R2=float(r2_score(y_true[:,i], y_pred[:,i])),\n",
    "        )\n",
    "    return pd.DataFrame(out).T\n",
    "\n",
    "if MODE == \"2h\":\n",
    "    MAX_ESTIMATORS, MAX_DEPTH, LEARNING_RATE = 350, 6, 0.07\n",
    "elif MODE == \"5h\":\n",
    "    MAX_ESTIMATORS, MAX_DEPTH, LEARNING_RATE = 700, 7, 0.05\n",
    "else:\n",
    "    MAX_ESTIMATORS, MAX_DEPTH, LEARNING_RATE = 1200, 8, 0.045\n",
    "\n",
    "def make_ridge(alpha=3.0):\n",
    "    return Ridge(alpha=alpha, fit_intercept=True, random_state=SEED)\n",
    "\n",
    "def make_xgb(n_estimators=MAX_ESTIMATORS, max_depth=MAX_DEPTH, learning_rate=LEARNING_RATE, subsample=0.9, colsample_bytree=0.9):\n",
    "    params=dict(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        params.update(dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", device=\"cuda\"))\n",
    "        print(\"‚ö° XGB: Using GPU\")\n",
    "    else:\n",
    "        params.update(dict(tree_method=\"hist\", predictor=\"cpu_predictor\", device=\"cpu\"))\n",
    "        print(\"‚ö†Ô∏è XGB: Using CPU\")\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "def eval_on_split(model, Xtr, ytr_log, Xev, yev_log, label=\"eval\"):\n",
    "    with timer(f\"fit {label}\"):\n",
    "        model.fit(Xtr, ytr_log)\n",
    "    yhat_log = model.predict(Xev)\n",
    "    yhat = np.expm1(yhat_log)\n",
    "    ytrue = np.expm1(yev_log)\n",
    "    mf = metrics_frame(ytrue, yhat, [\"damage_property\", \"damage_crops\"])\n",
    "    print(f\"\\nüìä {label} results:\\n{mf}\\n\")\n",
    "    return mf, yhat\n",
    "\n",
    "val_results = {}\n",
    "print(\"üöÄ Running first‚Äëpass‚Ä¶\")\n",
    "for name, model in {\"ridge\": make_ridge(), \"xgb\": make_xgb()}.items():\n",
    "    mf, _ = eval_on_split(model, Xt_train, y_train_log, Xt_valid, y_valid_log, f\"{name} (validation)\")\n",
    "    val_results[name] = mf\n",
    "\n",
    "path = OUTDIR / \"validation_metrics_first_pass.json\"\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump({k: v.to_dict(orient=\"index\") for k, v in val_results.items()}, f, indent=2)\n",
    "print(\"‚úÖ Saved first-pass metrics to\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80483e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Step 7 ‚Äî Final GPU XGB Training\n",
      "‚ö° Training final model on GPU\n",
      "‚è±Ô∏è Fit final XGB model (GPU-accelerated) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:37:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\ducan\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [14:37:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 7) Final GPU XGBoost Model (Full Training) ===\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"üîß Step 7 ‚Äî Final GPU XGB Training\")\n",
    "\n",
    "def make_xgb_final():\n",
    "    params = dict(\n",
    "        n_estimators=1500 if MODE=='full' else (700 if MODE=='5h' else 500),\n",
    "        max_depth=8 if MODE!='2h' else 6,\n",
    "        learning_rate=0.045 if MODE!='2h' else 0.06,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        params.update(dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\", device=\"cuda\"))\n",
    "        print(\"‚ö° Training final model on GPU\")\n",
    "    else:\n",
    "        params.update(dict(tree_method=\"hist\", predictor=\"cpu_predictor\", device=\"cpu\"))\n",
    "        print(\"‚ö†Ô∏è GPU not available ‚Äî training on CPU\")\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "final_model = make_xgb_final()\n",
    "with timer(\"Fit final XGB model (GPU-accelerated)\"):\n",
    "    final_model.fit(Xt_train, y_train_log)\n",
    "\n",
    "print(\"‚úÖ Final GPU XGB model trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41faf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 8) SHAP Analysis (GPU TreeSHAP) ===\n",
    "import shap\n",
    "\n",
    "print(\"üîß Step 8 ‚Äî SHAP Global Interpretation\")\n",
    "\n",
    "feature_names = np.load(\"feature_names.npy\", allow_pickle=True)\n",
    "\n",
    "shap_sample_size = min(4000, Xt_train.shape[0])\n",
    "shap_idx = np.random.choice(len(Xt_train), shap_sample_size, replace=False)\n",
    "Xt_shap = Xt_train[shap_idx]\n",
    "\n",
    "with timer(\"Compute SHAP values (GPU)\"):\n",
    "    explainer = shap.TreeExplainer(final_model, feature_perturbation=\"tree_path_dependent\")\n",
    "    shap_values = explainer.shap_values(Xt_shap)\n",
    "\n",
    "np.save(OUTDIR / \"shap_values.npy\", shap_values)\n",
    "np.save(OUTDIR / \"shap_sample.npy\", Xt_shap)\n",
    "print(\"‚úÖ SHAP values computed and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 9) Final Predictions + Quantile Intervals (GPU) ===\n",
    "print(\"üîß Step 9 ‚Äî Final Predictions & Uncertainty\")\n",
    "\n",
    "with timer(\"Predict on validation set (GPU)\"):\n",
    "    yhat_log_valid = final_model.predict(Xt_valid)\n",
    "    yhat_valid = np.expm1(yhat_log_valid)\n",
    "    ytrue_valid = np.expm1(y_valid_log)\n",
    "\n",
    "resid = ytrue_valid - yhat_valid\n",
    "QUANTILES = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "qvals = np.quantile(resid, QUANTILES, axis=0)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"ytrue_property\": ytrue_valid[:,0],\n",
    "    \"ypred_property\": yhat_valid[:,0],\n",
    "    \"ytrue_crop\": ytrue_valid[:,1],\n",
    "    \"ypred_crop\": yhat_valid[:,1],\n",
    "})\n",
    "\n",
    "for q, val in zip(QUANTILES, qvals):\n",
    "    results[f\"q{int(q*100)}_prop\"] = yhat_valid[:,0] + val[0]\n",
    "    results[f\"q{int(q*100)}_crop\"] = yhat_valid[:,1] + val[1]\n",
    "\n",
    "out_csv = OUTDIR / \"test_quantile_predictions.csv\"\n",
    "results.to_csv(out_csv, index=False)\n",
    "print(\"‚úÖ Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 10) Preview artifacts ===\n",
    "from pprint import pprint\n",
    "print(\"Artifacts in results/:\")\n",
    "pprint(sorted([p.name for p in OUTDIR.glob(\"*\")]))\n",
    "display(pd.read_csv(OUTDIR/\"test_quantile_predictions.csv\").head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
